
<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title> ICCV 2021 Workshop: Self-supervised Learning for Next-Generation Industry-level Autonomous Driving </title>

  <link rel="canonical" href="./index.html">
  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/style.css">
  <link rel="stylesheet" href="./theme/css/custom.css">


  <meta name="description" content="ICCV 2021 Workshop: Self-supervised Learning for Next-Generation Industry-level Autonomous Driving">
</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <!-- <div class="col-xs-2">
          <a href="./">
            <img class="img-fluid" src=./images/logo.png alt="ML4H: Machine Learning for Health">
          </a>
        </div> -->
        <div class="col-xs-10">
          <h1 class="title" style="width:950px"><a href="../">ICCV 2021 Workshop: Self-supervised Learning for Next-Generation Industry-level Autonomous Driving</a></h1>
          <br />
          <ul class="list-inline" style="width:max-content"> 
            <li class="list-inline-item"><a href="./index.html">Home</a></li>
            <li class="list-inline-item"><a href="./pages/call-for-participation.html">Call for Submissions</a></li>
            <li class="list-inline-item"><a href="./pages/schedule.html">Schedule</a></li>
            <li class="list-inline-item"><a href="./pages/speakers.html">Speakers</a></li>
            <li class="list-inline-item"><a href="./pages/organizers.html">Organizers</a></li>
            <li class="list-inline-item"><a href="./pages/Program Committee.html">Program Committee</a></li>
            <li class="list-inline-item"><a href="./pages/Accepted Paper.html">Accepted Papers</a></li>
            <li class="list-inline-item"><a href="./pages/challenge">Challenge</a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>
</h1>

<article class="article">
  <div class="content">
    <div class="container">

<div class="row">
<!-- <div class="col-md-7"> -->
    <p>
        <!-- <b>
            ML4H 2020: a workshop at
            <a href="https://neurips.cc">NeurIPS 2020</a>
        </b> <br> -->
   Online Workshop,  xxxx, 2021
   
    
    </p>
  <!-- <p>
    Workshop Webpage: <a href='https://neurips.cc/virtual/2020/protected/workshop_16146.html'>https://neurips.cc/virtual/2020/protected/workshop_16146.html</a> -->
  <!-- <pstyle="color:grey;">Workshop Webpage -->
  </p>
    
    <p>Self-supervised Learning for Next-generation Industry-level Autonomous Driving refers to a variety of studies that attempt to refresh the solutions for challenging real-world perception tasks by learning from unlabeled or semi-supervised large-scale collected data to incrementally self-train powerful recognition models. Thanks to the rise of large-scale annotated data sets and the advance in computing hardware, various supervised learning methods have significantly improved the performance in many problems (e.g. 2D detection, instance segmentation and 3D Lidar Detection) in the field of self-driving. However, these supervised learning approaches are notorious "data hungry", especially in the current autonomous driving fields. The performance of self-driving perception systems highly relies on the annotation scale of labeled bounding boxes and IDs, which makes them not practical in many real-world industrial applications. While the intuition is that a human driver can keep accumulating experiences from self-exploring the roads without any tutorâ€™s guidance, current CV solutions are still baby-sitted with extensive annotation efforts on every new scenario. To facilitate an industry-level autonomous driving system in the future, the desired visual recognition model should be equipped with the ability of self-exploring, self-training and self-adapting across diverse new-appearing geographies, streets, cities, weather conditions, object labels, viewpoints or abnormal scenarios. To address this problem, many recent efforts in self-supervised learning, large-scale pretraining, weakly supervised learning and incremental/continual learning have been made to improve the perception systems to deviate from traditional paths of supervised learning for self-driving solutions. Many research works have been devoted to related topics, leading to rapid growth of related publications in the top-tier conferences and journals such as CVPR, ICCV, ECCV, T-IP, and T-PAMI, albeit mostly evaluated on relatively small scale and highly curated datasets.</p>
    <p>We organize this workshop to investigate current ways of building next-generation industry level autonomous driving systems by resorting to self-supervised learning. We believe that this workshop will attract discussions from both industry and academics and create synergies to fields that focus on learning and pretraining from limited labels (one/few/zero-shot/self-supervised learning) along/beyond autonomous driving. </p>  
    <p>As part of the workshop, we also release a new Self-training Self-Driving (SSD) challenge, which to the best of our knowledge is the largest of its kind. It includes three competition tasks and contains 10 million 2D images and 1 million video frames collected from real-world driving scenarios. Note, multimodal information is available for the video data, where each frame is accompanied by 1 lidar image and 6 other view angles. This SSD challenge aims to provide a standard industry-level benchmark for examining the generalization and robustness ability of self-supervised/semi-supervised perception models on large-scale real-world self-driving applications.</p>
    <p>
      Regarding the viability of this workshop, the topic is practical, attractive and active. It is highly likely that many active researchers will attend this workshop. It is related to yet still clearly different from past workshops as explained below.  To facilitate large-scale real-world self-driving, a lot of research is still required from the community and we see this topic underrepresented at the main conference. In addition, we have gotten confirmation from many renowned professors and researchers in this area and they are either glad to give a keynote speech (as listed in the program) or kindly offer help. We believe this workshop will be a very successful one and it will indeed benefit the progress of this research area significantly.
    </p>
    <p>
      In this half-day workshop, we will have regular paper presentations, invited speakers, and technical benchmark challenges to present the current state of the art, as well as the limitations and future directions for computer vision in autonomous driving, arguably the most promising application of computer vision and AI in general.
    </p>
    <!-- <div></div> -->
<!-- </div> -->


    <!-- </div> -->

<!-- <div class="text"> -->
    <p>The workshop is expected to attract research on self-supervised, semi-supervised and self-training techniques for achieving industry-level autonomous driving solutions, which will cover but are not limited to the following topics: </p>
    <ul>
      <li>
        Self-supervised learning techniques
      </li>
      <li>
        Life-long/incremental visual recognition methods
      </li>
      <li>
        Weakly supervised learning algorithms
      </li>
      <li>
        One/few/zero shot learning for perception tasks in self-driving
      </li>
      <li>
        Learning in the presence of noisy data
      </li>
      <li>
        Domain adaptation
      </li>
      <li>
        Weakly supervised learning for 3D Lidar and 2D images
      </li>
      <li>
        Real world self-driving image applications, e.g. lane detection, anomaly detection, object semantic segmentation/detection/localization, scene parsing, etc.
      </li>
      <li>
        Real world self-driving image applications, e.g. lane detection, anomaly detection, object semantic segmentation/detection/localization, scene parsing, etc.
      </li>
      <li>
        Vision-based localization and tracking
      </li>
      <li>
        Safety/explainability/robustness for self-driving cars in the abovementioned settings
      </li>
    </ul>    
    <!-- <p>Apply for a <a href="https://forms.gle/SEc4Nj3RnmN6GN8e8">Travel Grant</a>.</p> -->



</div>
  </div>
</article>
    </div>
  </div>

  <!-- <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">


          <li class="list-inline-item"><a href="./categories.html">Categories</a></li>

        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          <a href="https://github.com/ml4health/ml4health.github.io">
          source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer> -->
</body>

</html>
